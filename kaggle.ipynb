{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deepi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deepi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeepi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manalytics vidhya\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcourses1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example of dataset structure\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\deepi\\analytics vidhya\\courses1.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "# Example of dataset structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9 entries, 0 to 8\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Title        9 non-null      object\n",
      " 1   Description  9 non-null      object\n",
      " 2   Curriculum   9 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 348.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to clean a specified column from a dataset by removing special characters and converting the text to lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formating_col(col_name):\n",
    "    elt=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        a=re.sub(r'[^\\w\\s]', ' ',data[col_name][i])\n",
    "        a=a.lower()\n",
    "        elt.append(a)\n",
    "    \n",
    "    return elt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 'Title', 'Description', and 'Curriculum' columns into a new 'tags' column\n",
    "data[\"tags\"]=data['Title']+data['Description']+data['Curriculum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize an empty list to store the cleaned 'tags'\n",
    "tags = []\n",
    "\n",
    "# Loop through each row in the 'data' DataFrame\n",
    "for i in range(data.shape[0]):\n",
    "    \n",
    "    # Split the text in the 'tags' column of the current row by spaces into individual words\n",
    "    # Remove any word that is a stopword (commonly used words like 'the', 'is', etc.) using NLTK's stopword list\n",
    "    elt = [word for word in data['tags'][i].split(\" \") if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join the remaining words back into a string, separated by a space\n",
    "    elt = \" \".join(elt)\n",
    "    \n",
    "    # Append the cleaned string to the 'tags' list\n",
    "    tags.append(elt)\n",
    "\n",
    "# tags=[]\n",
    "# for i in range(data.shape[0]):\n",
    "#     elt=[word for word in data['tags'][i].split(\" \") if word not in stopwords.words('english')]\n",
    "#     elt=\" \".join(elt)\n",
    "#     tags.append(elt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Curriculum</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Improving Real World RAG Systems: Key Challeng...</td>\n",
       "      <td>This course explores the key challenges in bui...</td>\n",
       "      <td>Improving Real World RAG System\\nIntroduction ...</td>\n",
       "      <td>Improving Real World RAG Systems: Key Challeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Framework to Choose the Right LLM for your Bus...</td>\n",
       "      <td>This course will guide you through the process...</td>\n",
       "      <td>Introduction\\nIntroduction\\n2\\nIt's an LLM Wor...</td>\n",
       "      <td>Framework to Choose the Right LLM for your Bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative AI - A Way of Life</td>\n",
       "      <td>This course is a transformative journey tailor...</td>\n",
       "      <td>Introduction to Generative AI\\nFundamentals of...</td>\n",
       "      <td>Generative AI - A Way of LifeThis course is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Building LLM Applications using Prompt Enginee...</td>\n",
       "      <td>This course will provide you with a hands-on u...</td>\n",
       "      <td>How to build diffferent LLM AppIications?\\nInt...</td>\n",
       "      <td>Building LLM Applications using Prompt Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging and Boosting ML Algorithms - Free Course</td>\n",
       "      <td>This course will provide you with a hands-on u...</td>\n",
       "      <td>Bagging\\nResources to be used in this course\\n...</td>\n",
       "      <td>Bagging and Boosting ML Algorithms - Free Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Understanding Linear Regression - Free Course</td>\n",
       "      <td>This free course will help you understand the ...</td>\n",
       "      <td>Linear Regression\\nIntroduction to the Problem...</td>\n",
       "      <td>Understanding Linear Regression - Free CourseT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Building Your First Computer Vision Model - Fr...</td>\n",
       "      <td>This course will help you gain a deep understa...</td>\n",
       "      <td>Introduction to Computer Vision\\nPixel Perfect...</td>\n",
       "      <td>Building Your First Computer Vision Model - Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MidJourney: From Inspiration to Implementation...</td>\n",
       "      <td>This course will provide you with a practical ...</td>\n",
       "      <td>MidJourney\\nMidJourney - Storm _ Story\\nMidJou...</td>\n",
       "      <td>MidJourney: From Inspiration to Implementation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Building Smarter LLMs with Mamba and State Spa...</td>\n",
       "      <td>Unlock the Power of State Space Models (SSM) l...</td>\n",
       "      <td>Course Overview\\nCourse Overview\\n2\\nAn Altern...</td>\n",
       "      <td>Building Smarter LLMs with Mamba and State Spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Improving Real World RAG Systems: Key Challeng...   \n",
       "1  Framework to Choose the Right LLM for your Bus...   \n",
       "2                      Generative AI - A Way of Life   \n",
       "3  Building LLM Applications using Prompt Enginee...   \n",
       "4   Bagging and Boosting ML Algorithms - Free Course   \n",
       "5      Understanding Linear Regression - Free Course   \n",
       "6  Building Your First Computer Vision Model - Fr...   \n",
       "7  MidJourney: From Inspiration to Implementation...   \n",
       "8  Building Smarter LLMs with Mamba and State Spa...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  This course explores the key challenges in bui...   \n",
       "1  This course will guide you through the process...   \n",
       "2  This course is a transformative journey tailor...   \n",
       "3  This course will provide you with a hands-on u...   \n",
       "4  This course will provide you with a hands-on u...   \n",
       "5  This free course will help you understand the ...   \n",
       "6  This course will help you gain a deep understa...   \n",
       "7  This course will provide you with a practical ...   \n",
       "8  Unlock the Power of State Space Models (SSM) l...   \n",
       "\n",
       "                                          Curriculum  \\\n",
       "0  Improving Real World RAG System\\nIntroduction ...   \n",
       "1  Introduction\\nIntroduction\\n2\\nIt's an LLM Wor...   \n",
       "2  Introduction to Generative AI\\nFundamentals of...   \n",
       "3  How to build diffferent LLM AppIications?\\nInt...   \n",
       "4  Bagging\\nResources to be used in this course\\n...   \n",
       "5  Linear Regression\\nIntroduction to the Problem...   \n",
       "6  Introduction to Computer Vision\\nPixel Perfect...   \n",
       "7  MidJourney\\nMidJourney - Storm _ Story\\nMidJou...   \n",
       "8  Course Overview\\nCourse Overview\\n2\\nAn Altern...   \n",
       "\n",
       "                                                tags  \n",
       "0  Improving Real World RAG Systems: Key Challeng...  \n",
       "1  Framework to Choose the Right LLM for your Bus...  \n",
       "2  Generative AI - A Way of LifeThis course is a ...  \n",
       "3  Building LLM Applications using Prompt Enginee...  \n",
       "4  Bagging and Boosting ML Algorithms - Free Cour...  \n",
       "5  Understanding Linear Regression - Free CourseT...  \n",
       "6  Building Your First Computer Vision Model - Fr...  \n",
       "7  MidJourney: From Inspiration to Implementation...  \n",
       "8  Building Smarter LLMs with Mamba and State Spa...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs stemming, which reduces each word in the tags column to its base form, allowing for more efficient text processing. Stemming is typically less precise than lemmatization but can be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\",\n",
       " \"improving real world rag systems: key challenges & practical solutionsthis course explores key challenges building real-world retrieval-augmented generation (rag) systems provides practical solutions. topics include improving data retrieval, dealing hallucinations, context selection, optimizing system performance using advanced prompting, retrieval strategies, evaluation techniques. through hands-on demos, gain insights better chunking, embedding models, agentic rag systems robust, real-world applications.improving real world rag system\\nintroduction rag systems\\nresources\\nrag system challenges practical solutions\\nhands-on: solution missing content rag\\nother key challenges\\npractical solutions\\nhands-on: solution missed top ranked, not context, not extracted _ incorrect specificityhands-on- solution missed\\nwrong format problem solution\\nhands-on: solution wrong format\\nincomplete problem solution\\nhyde\\nother practical solutions recent research pap framework choose right llm businessthis course guide process selecting suitable large language model (llm) various business needs. by examining factors accuracy, cost, scalability, integration, understand different llms perform specific scenarios, customer support healthcare strategy development. the course emphasizes practical decision-making real-world case studies, helping businesses navigate rapidly evolving llm landscape effectively.introduction\\nintroduction\\n2\\nit's llm world!\\nit's llm world!\\n3\\nunderstand your business\\nunderstand your business\\n4\\nframework choose right llm\\nframework choose right llm\\n5\\ncase studies\\ncase studies\\n6\\nconclusion\\nconclus generative ai - a way lifethis course transformative journey tailored beginners delves ai-powered text image generation using leading tools like chatgpt, microsoft copilot, dallúe3. learn practical applications across industries, ethical considerations, best practices. whether content creator, business innovator, ai enthusiast, gain expertise harness generative ai's full potential drive innovation field.introduction generative ai\\nfundamentals generative ai\\nwhat generative ai?\\nhow generative ai work?\\nexploring potential generative ai\\ngenai pinnacle program\\nhands on: let?s get generating!\\n2\\ntext generation using generative ai\\nan overview text generation\\nwhat chatgpt?\\nworking chatgpt\\nworking chatgpt plus\\nworking bing chat\\nbreaking bard\\nsponsored ad\\nlearning art prompting\\ncreating chatbot\\nethics best practices\\n3\\nimage generation using generative ai\\nintroduction image generation\\nexploring potential image generation\\nworking free image generation tools\\nworking clipdrop\\nworking bing image creator\\nworking firefly\\nworking paid image generative tools\\nworking paid image generative tools dreamstudio\\nworking dalle-2\\nworking midjourney\\nsponsored ad\\nprompting way art\\naccomplishing tasks image generation\\ne ethics effici building llm applications using prompt engineering -this course provide hands-on understanding building llm applications mastering prompt engineering techniques. by end course, proficient implementing fine-tuning techniques enhance generative ai model performance. you'll learn apply various prompting methods build chatbots enterprise data, equipping skills improve conversational ai systems real-world projects.how build diffferent llm appiications?\\nintroduction building different llm applications\\nprompt engineering\\nretrieval augmented generation\\nfinetuning llms\\ntraining llms scratch\\nquiz\\n2\\ngetting started prompt engineering\\nintroduction prompt engineering\\nset machine prompt engineering\\nprompt engineering chatgpt api\\nenabling conversation chatgpt api\\nquiz\\n3\\nunderstanding different prompt engineering techniques\\nintroduction understanding different prompt engineering techniques\\nfew shot prompting\\none shot prompting\\nzero shot prompting\\nquiz\\nassign bagging boosting ml algorithms - free coursethis course provide hands-on understanding bagging boosting techniques machine learning. by end course, proficient implementing tuning ensemble methods enhance model performance. you'll learn apply algorithms like random forest, adaboost, gradient boosting real-world dataset, equipping skills improve predictive accuracy robustness projects.bagging\\nresources used course\\nproblem statement\\nunderstanding ensemble learning\\nintroducing bagging algorithms\\nhands-on bagging meta estimator\\nintroduction random forest\\nunderstanding out-of-bag score\\nrandom forest vs classical bagging vs decision tree\\nproject\\n2\\nboosting\\nintroduction boosting\\nadaboost step-by-step explanation\\nhands-on - adaboost\\ngradient boosting machines (gbm)\\nhands-on gradient boost\\nother algo (xgboost, lightboost. catboost)\\nproject: anova insur understanding linear regression - free coursethis free course help understand fundamentals linear regression straightforward manner. by end course, able build predictive models using linear regression techniques. with carefully curated list resources exercises, course serves comprehensive guide mastering linear regression. linear regression\\nintroduction problem statement\\nresources course\\nintroduction linear regression\\nsignificance slope intercept linear regression\\nhow model decides the best-fit line\\nlet?s build simple linear regression model\\nmodel understanding using descriptive approach\\nmodel understanding using descriptive approach - ii\\nmodel building using predictive approach\\nquiz: linear regress building your first computer vision model - free coursethis course help gain deep understanding computer vision build advanced cv models using pytorch framework. with carefully curated list resources exercises, course guide becoming computer vision expert. master techniques build convolutional neural networks, classify images.introduction computer vision\\npixel perfect - decoding images\\nunderstanding cnn - convolutional layer\\nhands - image processing techniques\\nunderstanding cnn - striding pooling\\nunderstanding cnn - pooling layer\\nunderstanding alexnet building cnn model\\nquiz midjourney: from inspiration implementation - free coursethis course provide practical understanding midjourney tools. by end course, able utilize midjourney effectively explore alternative tools creative projects. you'll learn draw inspiration, use midjourney's features, understand applications engaging lessons.midjourney\\nmidjourney - storm _ story\\nmidjourney - inspiration\\nmidjourney - how use\\nmidjourney alternatives\\nquiz building smarter llms mamba state space modelunlock power state space models (ssm) like mamba comprehensive course designed ai professionals, data scientists, nlp enthusiasts. master art integrating ssm deep learning, unravel complexities models like mamba, elevate understanding generative ai's newest innovative models. this course designed equip skills needed understand cutting-edge ai models work, making proficient latest ai techniques architectures.course overview\\ncourse overview\\n2\\nan alternative transformers\\nare rnns solution\\nthe problem transformers\\n3\\nunderstanding state space models\\nwhat state space model?\\nthe discrete representation\\nthe recurrent representation\\nthe convolution representation\\nthe three representations\\nthe importance a matrix\\n4\\nmamba - a selective state space model\\nwhat problem attempt solve?\\nselectively retaining information\\nspeeding up computations\\nexploring mamba block\\njamba - mixing mamba transform\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words=[]\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(data.shape[0]):\n",
    "    elt=[stemmer.stem(word) for word in tags]\n",
    "    elt=\" \".join(elt)\n",
    "    stemmed_words.append(elt)   \n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tags']=tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " extracts and renames two columns (\"Title\" and \"tags\") from the data DataFrame to create a simplified, more focused DataFrame (useful_df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefull_df=data[[\"Title\", 'tags']]\n",
    "usefull_df.columns=['Title','Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Improving Real World RAG Systems: Key Challeng...</td>\n",
       "      <td>Improving Real World RAG Systems: Key Challeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Framework to Choose the Right LLM for your Bus...</td>\n",
       "      <td>Framework Choose Right LLM BusinessThis course...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative AI - A Way of Life</td>\n",
       "      <td>Generative AI - A Way LifeThis course transfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Building LLM Applications using Prompt Enginee...</td>\n",
       "      <td>Building LLM Applications using Prompt Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagging and Boosting ML Algorithms - Free Course</td>\n",
       "      <td>Bagging Boosting ML Algorithms - Free CourseTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Understanding Linear Regression - Free Course</td>\n",
       "      <td>Understanding Linear Regression - Free CourseT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Building Your First Computer Vision Model - Fr...</td>\n",
       "      <td>Building Your First Computer Vision Model - Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MidJourney: From Inspiration to Implementation...</td>\n",
       "      <td>MidJourney: From Inspiration Implementation - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Building Smarter LLMs with Mamba and State Spa...</td>\n",
       "      <td>Building Smarter LLMs Mamba State Space ModelU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Improving Real World RAG Systems: Key Challeng...   \n",
       "1  Framework to Choose the Right LLM for your Bus...   \n",
       "2                      Generative AI - A Way of Life   \n",
       "3  Building LLM Applications using Prompt Enginee...   \n",
       "4   Bagging and Boosting ML Algorithms - Free Course   \n",
       "5      Understanding Linear Regression - Free Course   \n",
       "6  Building Your First Computer Vision Model - Fr...   \n",
       "7  MidJourney: From Inspiration to Implementation...   \n",
       "8  Building Smarter LLMs with Mamba and State Spa...   \n",
       "\n",
       "                                                Tags  \n",
       "0  Improving Real World RAG Systems: Key Challeng...  \n",
       "1  Framework Choose Right LLM BusinessThis course...  \n",
       "2  Generative AI - A Way LifeThis course transfor...  \n",
       "3  Building LLM Applications using Prompt Enginee...  \n",
       "4  Bagging Boosting ML Algorithms - Free CourseTh...  \n",
       "5  Understanding Linear Regression - Free CourseT...  \n",
       "6  Building Your First Computer Vision Model - Fr...  \n",
       "7  MidJourney: From Inspiration Implementation - ...  \n",
       "8  Building Smarter LLMs Mamba State Space ModelU...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usefull_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are the keys and Tags are the values, and then saves this dictionary as a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has been saved in the file json_test_api.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "keys=[usefull_df['Title'][i] for i in range(len(data))]\n",
    "values=[usefull_df['Tags'][i] for i in range(len(data))]\n",
    "json_test_api = dict(zip(keys, values))?\n",
    "\n",
    "\n",
    "\n",
    "filename='json_test_api.json'\n",
    "# Sauvegarde du dictionnaire dans un fichier JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(json_test_api, file)\n",
    "    \n",
    "print(f\"The dictionary has been saved in the file {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates two matrices representing the text data (the tags column) using TF-IDF and Count Vectorizer. Then it calculates the cosine similarity between the documents based on both representations. The result (similarity_tfidf and similarity_cv) is a matrix of similarity scores, where higher scores indicate more similar documents.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['tags']).toarray()\n",
    "\n",
    "cv=CountVectorizer(max_features=3522,stop_words='english')\n",
    "cv=cv.fit_transform(data['tags']).toarray()\n",
    "\n",
    "similarity_tfidf = cosine_similarity(X)\n",
    "similarity_cv = cosine_similarity(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation_cv(title):\n",
    "    course = usefull_df[usefull_df['Title'] == title.lower()]\n",
    "    if len(course)==0:\n",
    "        print(\"This course does not exist\")\n",
    "    else:\n",
    "        course_index=course.index[0]\n",
    "        distances = similarity_cv[course_index]\n",
    "        course_list = sorted(list(enumerate(distances)),reverse=True, key=lambda x:x[1])[1:11]\n",
    "\n",
    "        for i in course_list:\n",
    "            print(usefull_df.iloc[i[0]].Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommandation_tfidf(title):\n",
    "    course = usefull_df[usefull_df['Title'] == title.lower()]\n",
    "    if len(course)==0:\n",
    "        print(\"This course does not exist\")\n",
    "    else:\n",
    "        course_index=course.index[0]\n",
    "        distances = similarity_tfidf[course_index]\n",
    "        course_list = sorted(list(enumerate(distances)),reverse=True, key=lambda x:x[1])[1:11]\n",
    "\n",
    "        for i in course_list:\n",
    "            print(usefull_df.iloc[i[0]].Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course does not exist\n"
     ]
    }
   ],
   "source": [
    "recommandation_tfidf(\"bagging and boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course does not exist\n"
     ]
    }
   ],
   "source": [
    "recommandation_cv(\"bagging and boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a dictionary mapping each course index (0, 1, 2, etc.) to its corresponding course title from the usefull_df DataFrame. It then saves this dictionary to a JSON file named index_course_name.json for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has been saved in the file index_course_name.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "keys=[i for i in range(len(data))]\n",
    "values=[usefull_df['Title'][i] for i in range(len(data))]\n",
    "index_course_name = dict(zip(keys, values))\n",
    "\n",
    "filename='index_course_name.json'\n",
    "# Sauvegarde du dictionnaire dans un fichier JSON\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(index_course_name, file)\n",
    "    \n",
    "print(f\"The dictionary has been saved in the file {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(max_features=3522,stop_words='english')\n",
    "cv_fited=vectorizer.fit(usefull_df['Title'])#(data['tags'])#.toarray()\n",
    "import pickle\n",
    "with open('vectorizer_fited.pkl','wb') as file:\n",
    "    pickle.dump(cv_fited,file)\n",
    "\n",
    "print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "description_transformed=cv_fited.transform(usefull_df['Tags']).toarray()\n",
    "\n",
    "import pickle\n",
    "with open('description_fit_transformed.pkl','wb') as file:\n",
    "    pickle.dump(description_transformed,file)\n",
    "\n",
    "print(\"File saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"vectorizer_fited.pkl\", 'rb') as file:\n",
    "    loaded_vectorizer = pickle.load(file)\n",
    "\n",
    "with open(\"description_fit_transformed.pkl\", 'rb') as file:\n",
    "    loaded_description = pickle.load(file)\n",
    "    \n",
    "with open(\"index_course_name.json\", 'r') as file:\n",
    "    loaded_dictionary = json.load(file)\n",
    "    \n",
    "\n",
    "def recommandation_deploy(title):\n",
    "    title=title.split(\" \")\n",
    "    title=loaded_vectorizer.transform(title)\n",
    "    similarities = cosine_similarity(title,loaded_description)\n",
    "    indices_similaires = np.argsort(similarities[0])[::-1]\n",
    "    indices_similaires=indices_similaires[:10]\n",
    "    indices_similaires=[str(i) for i in indices_similaires]\n",
    "    # Obtenir les cours des livres recommandés\n",
    "    cours_recommandes = [loaded_dictionary[i] for i in indices_similaires]\n",
    "    #livres_recommandes = [loaded_dictionary[str(2605)] ]\n",
    "    print(\"recommended courses:\", cours_recommandes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended courses: ['Bagging and Boosting ML Algorithms - Free Course', 'Building Smarter LLMs with Mamba and State Space Model', 'MidJourney: From Inspiration to Implementation - Free Course', 'Building Your First Computer Vision Model - Free Course', 'Understanding Linear Regression - Free Course', 'Building LLM Applications using Prompt Engineering -', 'Generative AI - A Way of Life', 'Framework to Choose the Right LLM for your Business', 'Improving Real World RAG Systems: Key Challenges & Practical Solutions']\n"
     ]
    }
   ],
   "source": [
    "recommandation_deploy(\"bagging and boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommended courses: ['Building Smarter LLMs with Mamba and State Space Model', 'MidJourney: From Inspiration to Implementation - Free Course', 'Building Your First Computer Vision Model - Free Course', 'Understanding Linear Regression - Free Course', 'Bagging and Boosting ML Algorithms - Free Course', 'Building LLM Applications using Prompt Engineering -', 'Generative AI - A Way of Life', 'Framework to Choose the Right LLM for your Business', 'Improving Real World RAG Systems: Key Challenges & Practical Solutions']\n"
     ]
    }
   ],
   "source": [
    "recommandation_deploy(\"machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deepi\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 171, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     43\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am interested in data science and machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 44\u001b[0m recommended_courses \u001b[38;5;241m=\u001b[39m \u001b[43mrecommandation_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended courses:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recommended_courses)\n",
      "Cell \u001b[1;32mIn[50], line 27\u001b[0m, in \u001b[0;36mrecommandation_llm\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Tokenize and generate response from the model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Decode the generated output\u001b[39;00m\n\u001b[0;32m     30\u001b[0m recommendation_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1906\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1904\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1906\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_params\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1228\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1227\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1238\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 171, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the LLM model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace this with a more advanced model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load your course data\n",
    "with open(\"index_course_name.json\", 'r') as file:\n",
    "    loaded_dictionary = json.load(file)\n",
    "\n",
    "# Load the course descriptions or any relevant information\n",
    "with open(\"description_fit_transformed.pkl\", 'rb') as file:\n",
    "    loaded_description = pickle.load(file)\n",
    "\n",
    "# Create a function for generating recommendations\n",
    "def recommandation_llm(user_input):\n",
    "    # Prepare the input prompt for the LLM\n",
    "    prompt = f\"Based on the following courses:\\n{json.dumps(loaded_dictionary)}\\n\"\n",
    "    prompt += f\"User input: {user_input}\\n\"\n",
    "    prompt += \"Recommend me some courses based on the above input:\\n\"\n",
    "\n",
    "    # Tokenize and generate response from the model\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the generated output\n",
    "    recommendation_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract course recommendations (assuming the output is structured correctly)\n",
    "    recommended_courses = extract_recommendations(recommendation_text)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def extract_recommendations(text):\n",
    "    # Custom function to parse the LLM output for recommended course titles\n",
    "    # This could be tailored based on how you want to extract recommendations\n",
    "    return text.split(\"\\n\")  # Modify as necessary to match your expected output format\n",
    "\n",
    "# Example usage\n",
    "user_input = \"I am interested in data science and machine learning.\"\n",
    "recommended_courses = recommandation_llm(user_input)\n",
    "print(\"Recommended courses:\", recommended_courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended courses from LLM: ['Based on the following courses:', '{\"0\": \"Improving Real World RAG Systems: Key Challenges & Practical Solutions\", \"1\": \"Framework to Choose the Right LLM for your Business\", \"2\": \"Generative AI - A Way of Life\", \"3\": \"Building LLM Applications using Prompt Engineering -\", \"4\": \"Bagging and Boosting ML Algorithms - Free Course\", \"5\": \"Understanding Linear Regression - Free Course\", \"6\": \"Building Your First Computer Vision Model - Free Course\", \"7\": \"MidJourney: From Inspiration to Implementation - Free Course\", \"8\": \"Building Smarter LLMs with Mamba and State Space Model\"}', 'User input: machine learning', 'Recommend me some courses based on the above input:', 'Machine Learning', 'Machine Learning is a new field of research that has been gaining traction in the last few years. It is a field that has been gaining traction in the last few years. It is a field that has been gaining traction in the']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 50257 while Y.shape[1] == 43",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended courses from LLM:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recommended_courses_llm)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Optionally, get recommendations using cosine similarity (if embeddings are used)\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mrecommandation_deploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 52\u001b[0m, in \u001b[0;36mrecommandation_deploy\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommandation_deploy\u001b[39m(title):\n\u001b[0;32m     51\u001b[0m     title_embedding \u001b[38;5;241m=\u001b[39m get_embeddings(title)\n\u001b[1;32m---> 52\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     indices_similaires \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(similarities[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     54\u001b[0m     cours_recommandes \u001b[38;5;241m=\u001b[39m [loaded_dictionary[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_similaires]\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:214\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    210\u001b[0m         )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 50257 while Y.shape[1] == 43"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np  # Import numpy as it's used in the similarity function\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the LLM model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace this with a more advanced model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load your course data\n",
    "with open(\"index_course_name.json\", 'r') as file:\n",
    "    loaded_dictionary = json.load(file)\n",
    "\n",
    "# Load the course descriptions or any relevant information\n",
    "with open(\"description_fit_transformed.pkl\", 'rb') as file:\n",
    "    loaded_description = pickle.load(file)\n",
    "\n",
    "def recommandation_llm(user_input):\n",
    "    # Prepare the input prompt for the LLM\n",
    "    prompt = f\"Based on the following courses:\\n{json.dumps(loaded_dictionary)}\\n\"\n",
    "    prompt += f\"User input: {user_input}\\n\"\n",
    "    prompt += \"Recommend me some courses based on the above input:\\n\"\n",
    "\n",
    "    # Tokenize and generate response from the model\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Use max_new_tokens instead of max_length\n",
    "    outputs = model.generate(inputs, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output\n",
    "    recommendation_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract course recommendations\n",
    "    recommended_courses = extract_recommendations(recommendation_text)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def extract_recommendations(text):\n",
    "    # Custom function to parse the LLM output for recommended course titles\n",
    "    # Assuming the model output contains course titles listed\n",
    "    return [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "# Function to get recommendations based on the similarity of embeddings (optional)\n",
    "def recommandation_deploy(title):\n",
    "    title_embedding = get_embeddings(title)\n",
    "    similarities = cosine_similarity(title_embedding, loaded_description)\n",
    "    indices_similaires = np.argsort(similarities[0])[::-1][:10]\n",
    "    cours_recommandes = [loaded_dictionary[str(i)] for i in indices_similaires]\n",
    "    print(\"Recommended courses based on embeddings:\", cours_recommandes)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Tokenize the input text and prepare input tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Use torch.no_grad() to prevent gradient calculations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Pass the inputs as keyword arguments\n",
    "        # We need to access the hidden states using logits instead\n",
    "        embeddings = outputs.logits[:, :-1, :].mean(dim=1)  # Use mean pooling for embeddings\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Main execution\n",
    "user_input = input(\"Enter the course title you are interested in: \")\n",
    "recommended_courses_llm = recommandation_llm(user_input)\n",
    "print(\"Recommended courses from LLM:\", recommended_courses_llm)\n",
    "\n",
    "# Optionally, get recommendations using cosine similarity (if embeddings are used)\n",
    "recommandation_deploy(user_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended courses from LLM: ['Based on the following courses:', '{\"0\": \"Improving Real World RAG Systems: Key Challenges & Practical Solutions\", \"1\": \"Framework to Choose the Right LLM for your Business\", \"2\": \"Generative AI - A Way of Life\", \"3\": \"Building LLM Applications using Prompt Engineering -\", \"4\": \"Bagging and Boosting ML Algorithms - Free Course\", \"5\": \"Understanding Linear Regression - Free Course\", \"6\": \"Building Your First Computer Vision Model - Free Course\", \"7\": \"MidJourney: From Inspiration to Implementation - Free Course\", \"8\": \"Building Smarter LLMs with Mamba and State Space Model\"}', 'User input: machine learning', 'Recommend me some courses based on the above input:', 'Machine Learning', 'Machine Learning is a new field of research that has been gaining traction in the last few years. It is a field that has been gaining traction in the last few years. It is a field that has been gaining traction in the']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 768 while Y.shape[1] == 43",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended courses from LLM:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recommended_courses_llm)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Optionally, get recommendations using cosine similarity (if embeddings are used)\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mrecommandation_deploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[58], line 53\u001b[0m, in \u001b[0;36mrecommandation_deploy\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecommandation_deploy\u001b[39m(title):\n\u001b[0;32m     52\u001b[0m     title_embedding \u001b[38;5;241m=\u001b[39m get_embeddings(title)\n\u001b[1;32m---> 53\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_description\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     indices_similaires \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(similarities[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     55\u001b[0m     cours_recommandes \u001b[38;5;241m=\u001b[39m [loaded_dictionary[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_similaires]\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\deepi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:214\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    210\u001b[0m         )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 768 while Y.shape[1] == 43"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np  # Import numpy as it's used in the similarity function\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the LLM model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace this with a more advanced model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_for_generation = AutoModelForCausalLM.from_pretrained(model_name)  # Use this for generation\n",
    "model_for_embedding = AutoModel.from_pretrained(model_name)  # Use this for embeddings\n",
    "\n",
    "# Set padding token to EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load your course data\n",
    "with open(\"index_course_name.json\", 'r') as file:\n",
    "    loaded_dictionary = json.load(file)\n",
    "\n",
    "# Load the course descriptions or any relevant information\n",
    "with open(\"description_fit_transformed.pkl\", 'rb') as file:\n",
    "    loaded_description = pickle.load(file)\n",
    "\n",
    "def recommandation_llm(user_input):\n",
    "    # Prepare the input prompt for the LLM\n",
    "    prompt = f\"Based on the following courses:\\n{json.dumps(loaded_dictionary)}\\n\"\n",
    "    prompt += f\"User input: {user_input}\\n\"\n",
    "    prompt += \"Recommend me some courses based on the above input:\\n\"\n",
    "\n",
    "    # Tokenize and generate response from the model\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model_for_generation.generate(inputs, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output\n",
    "    recommendation_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract course recommendations\n",
    "    recommended_courses = extract_recommendations(recommendation_text)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def extract_recommendations(text):\n",
    "    # Custom function to parse the LLM output for recommended course titles\n",
    "    # Assuming the model output contains course titles listed\n",
    "    return [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "# Function to get recommendations based on the similarity of embeddings (optional)\n",
    "def recommandation_deploy(title):\n",
    "    title_embedding = get_embeddings(title)\n",
    "    similarities = cosine_similarity(title_embedding, loaded_description)\n",
    "    indices_similaires = np.argsort(similarities[0])[::-1][:10]\n",
    "    cours_recommandes = [loaded_dictionary[str(i)] for i in indices_similaires]\n",
    "    print(\"Recommended courses based on embeddings:\", cours_recommandes)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Tokenize the input text and prepare input tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Use torch.no_grad() to prevent gradient calculations\n",
    "    with torch.no_grad():\n",
    "        outputs = model_for_embedding(**inputs)  # Get hidden states\n",
    "        # Use the last hidden state for mean pooling\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling for embeddings\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Main execution\n",
    "user_input = input(\"Enter the course title you are interested in: \")\n",
    "recommended_courses_llm = recommandation_llm(user_input)\n",
    "print(\"Recommended courses from LLM:\", recommended_courses_llm)\n",
    "\n",
    "# Optionally, get recommendations using cosine similarity (if embeddings are used)\n",
    "recommandation_deploy(user_input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded_description: <class 'numpy.ndarray'>\n",
      "Sample contents of loaded_description: [[ 0  0  1  0  0  1  0  4  0  0  1  0  0  0  0  0  3  0  3  0  0  0  0  0\n",
      "   0  0  0  5  0  7  4  0  0  0  4  0  0  4  0  1  0  0  4]\n",
      " [ 0  0  0  0  0  0  3  0  3  0  2  0  3  0  0  0  0  0  0  0  0  7  1  0\n",
      "   0  0  1  1  0  0  1  0  3  0  0  0  0  0  0  0  0  0  3]\n",
      " [11  0  1  0  0  0  1  0  0  0  1  0  0  1 11  0  0  0  0  0  0  0  0  0\n",
      "   1  0  0  1  0  0  0  0  0  0  0  0  0  0  0  3  0  2  0]\n",
      " [ 2  0  3  0  0  3  0  0  0  0  2  9  0  0  1  0  0  0  0  0  0  4  2  0\n",
      "   0  0  1  0  9  0  1  0  0  0  0  0  0  1  3  1  0  0  1]\n",
      " [ 0  3  0  6  6  0  0  0  0  0  3  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   0  1  1  0  0  0  1  0  0  0  0  0  0  0  3  0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the LLM model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace this with a more advanced model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Assign the eos_token as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load your course data\n",
    "with open(\"index_course_name.json\", 'r') as file:\n",
    "    loaded_dictionary = json.load(file)\n",
    "\n",
    "# Load the course descriptions or any relevant information\n",
    "with open(\"description_fit_transformed.pkl\", 'rb') as file:\n",
    "    loaded_description = pickle.load(file)\n",
    "\n",
    "\n",
    "def recommandation_llm(user_input):\n",
    "    # Prepare the input prompt for the LLM\n",
    "    prompt = f\"Based on the following courses:\\n{json.dumps(loaded_dictionary)}\\n\"\n",
    "    prompt += f\"User input: {user_input}\\n\"\n",
    "    prompt += \"Recommend me some courses based on the above input:\\n\"\n",
    "\n",
    "    # Tokenize and generate response from the model\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Use max_new_tokens instead of max_length\n",
    "    outputs = model.generate(inputs, max_new_tokens=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode the generated output\n",
    "    recommendation_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract course recommendations\n",
    "    recommended_courses = extract_recommendations(recommendation_text)\n",
    "\n",
    "    return recommended_courses\n",
    "\n",
    "def extract_recommendations(text):\n",
    "    # Custom function to parse the LLM output for recommended course titles\n",
    "    return [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "def get_loaded_description_embeddings():\n",
    "    all_embeddings = []\n",
    "    for description in loaded_description:\n",
    "        # Ensure description is a string\n",
    "        if isinstance(description, str):\n",
    "            embedding = get_embeddings(description)  # Get embedding for each description\n",
    "            all_embeddings.append(embedding)\n",
    "        else:\n",
    "            print(f\"Skipping non-string description: {description}\")\n",
    "\n",
    "    if len(all_embeddings) == 0:\n",
    "        print(\"Warning: No valid descriptions found for embedding.\")\n",
    "        return torch.empty(0)  # Return an empty tensor if no embeddings were collected\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)  # Concatenate to create a single tensor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommandation_deploy(title_vector):\n",
    "    title_vector = title_vector.reshape(1, -1)  # Reshape to match dimensions for cosine similarity\n",
    "    # Compute cosine similarity directly with the loaded description features\n",
    "    similarities = cosine_similarity(title_vector, loaded_description)\n",
    "\n",
    "    # Sort by similarity score\n",
    "    recommended_indices = similarities.argsort()[0][::-1]  # Get indices of most similar courses\n",
    "    recommended_courses = [loaded_description[i] for i in recommended_indices if i < len(loaded_description)]\n",
    "\n",
    "    print(\"Recommended courses based on numerical features:\", recommended_courses)\n",
    "\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(title_embedding.numpy(), loaded_description_embeddings.numpy())\n",
    "    indices_similaires = np.argsort(similarities[0])[::-1][:10]\n",
    "    cours_recommandes = [loaded_dictionary[str(i)] for i in indices_similaires]\n",
    "    print(\"Recommended courses based on embeddings:\", cours_recommandes)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Tokenize the input text and prepare input tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Use torch.no_grad() to prevent gradient calculations\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)  # Set output_hidden_states to True\n",
    "        # Get the last hidden states\n",
    "        hidden_states = outputs.hidden_states  # This is a tuple containing all hidden states\n",
    "        embeddings = hidden_states[-1].mean(dim=1)  # Use mean pooling for embeddings\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Main execution\n",
    "user_input = input(\"Enter the course title you are interested in: \")\n",
    "recommended_courses_llm = recommandation_llm(user_input)\n",
    "print(\"Recommended courses from LLM:\", recommended_courses_llm)\n",
    "\n",
    "# Optionally, get recommendations using cosine similarity (if embeddings are used)\n",
    "recommandation_deploy(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to get embeddings for course titles/descriptions\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token's representation as the embedding\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Combine title and description for course representation\n",
    "data['tags'] = data['Title'] + \" \" + data['Description']\n",
    "\n",
    "# Compute embeddings for each course\n",
    "data['embeddings'] = data['tags'].apply(lambda x: get_embeddings(x, model, tokenizer))\n",
    "\n",
    "# Stack the embeddings into a 2D array\n",
    "embeddings = np.stack(data['embeddings'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title\n",
      "5      Understanding Linear Regression - Free Course\n",
      "4   Bagging and Boosting ML Algorithms - Free Course\n",
      "6  Building Your First Computer Vision Model - Fr...\n",
      "0  Improving Real World RAG Systems: Key Challeng...\n",
      "2                      Generative AI - A Way of Life\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_course(input_text, embeddings, data, model, tokenizer, top_n=5):\n",
    "    # Get embeddings for the input course\n",
    "    input_embedding = get_embeddings(input_text, model, tokenizer)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity([input_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get indices of the top_n most similar courses\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    \n",
    "    # Return the most similar courses\n",
    "    return data.iloc[top_indices][['Title']]\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Introduction to Data Science\"\n",
    "recommended_courses = recommend_course(input_text, embeddings, data, model, tokenizer, top_n=5)\n",
    "print(recommended_courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming embeddings, data, model, and tokenizer are already defined\n",
    "\n",
    "def recommend_course(input_text):\n",
    "    # Get embeddings for the input course\n",
    "    input_embedding = get_embeddings(input_text, model, tokenizer)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity([input_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get indices of the top_n most similar courses\n",
    "    top_indices = np.argsort(similarities)[-5:][::-1]\n",
    "    \n",
    "    # Return the most similar courses\n",
    "    return data.iloc[top_indices][['Title']].to_dict(orient='records')\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=recommend_course, \n",
    "    inputs=\"text\", \n",
    "    outputs=\"json\",\n",
    "    title=\"Course Recommendation System\",\n",
    "    description=\"Enter a course title or description, and the system will recommend similar courses.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming embeddings, data, model, and tokenizer are already defined\n",
    "\n",
    "def recommend_course(input_text):\n",
    "    # Get embeddings for the input course\n",
    "    input_embedding = get_embeddings(input_text, model, tokenizer)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity([input_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get indices of the top_n most similar courses\n",
    "    top_indices = np.argsort(similarities)[-5:][::-1]\n",
    "    \n",
    "    # Get the course titles of the most similar courses\n",
    "    course_titles = data.iloc[top_indices]['Title'].tolist()\n",
    "    \n",
    "    # Format output as bullet points\n",
    "    formatted_output = \"\\n\".join([f\"- {title}\" for title in course_titles])\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# Gradio Interface with styling\n",
    "css = \"\"\"\n",
    "    #input_text {font-size: 16px; padding: 10px; border-radius: 5px;}\n",
    "    #output_text {font-size: 16px; padding: 10px; border-radius: 5px; background-color: #f9f9f9;}\n",
    "    .gr-button {font-size: 16px; padding: 10px 20px; border-radius: 5px;}\n",
    "\"\"\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=recommend_course,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter course title or description here...\", label=\"Course Input\"),\n",
    "    outputs=gr.Textbox(label=\"Recommended Courses\"),\n",
    "    title=\"Course Recommendation System\",\n",
    "    description=\"Enter a course title or description, and the system will recommend similar courses.\",\n",
    "    css=css\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
