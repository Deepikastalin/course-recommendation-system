{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Improving Real World RAG Systems: Key Challeng...   \n",
      "1  Framework to Choose the Right LLM for your Bus...   \n",
      "2                      Generative AI - A Way of Life   \n",
      "3  Building LLM Applications using Prompt Enginee...   \n",
      "4   Bagging and Boosting ML Algorithms - Free Course   \n",
      "\n",
      "                                         Description  \\\n",
      "0  This course explores the key challenges in bui...   \n",
      "1  This course will guide you through the process...   \n",
      "2  This course is a transformative journey tailor...   \n",
      "3  This course will provide you with a hands-on u...   \n",
      "4  This course will provide you with a hands-on u...   \n",
      "\n",
      "                                          Curriculum  \n",
      "0  Improving Real World RAG System\\nIntroduction ...  \n",
      "1  Introduction\\nIntroduction\\n2\\nIt's an LLM Wor...  \n",
      "2  Introduction to Generative AI\\nFundamentals of...  \n",
      "3  How to build diffferent LLM AppIications?\\nInt...  \n",
      "4  Bagging\\nResources to be used in this course\\n...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (example with a CSV file)\n",
    "#df = pd.read_csv(\"courses1.csv\", encoding='utf-8', errors='ignore')  # Skips invalid characters\n",
    "df = pd.read_csv(r\"C:\\Users\\deepi\\analytics vidhya\\courses1.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "# Example of dataset structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 28.3580379486084\n",
      "Epoch 1, Loss: 28.02680015563965\n",
      "Epoch 1, Loss: 28.00735855102539\n",
      "Epoch 2, Loss: 24.201677322387695\n",
      "Epoch 2, Loss: 22.90985107421875\n",
      "Epoch 2, Loss: 20.708946228027344\n",
      "Epoch 3, Loss: 17.513511657714844\n",
      "Epoch 3, Loss: 18.393674850463867\n",
      "Epoch 3, Loss: 17.59160804748535\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Example function to create the input prompt\n",
    "def create_prompt(title, description, curriculum):\n",
    "    # Define the prompt structure\n",
    "    prompt = f\"Course Title: {title}\\nDescription: {description}\\nCurriculum: {curriculum}\\n\"\n",
    "    prompt += \"Summarize this course or recommend it based on the description.\"\n",
    "    return prompt\n",
    "\n",
    "# Dataset class for handling course data\n",
    "class CourseDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title = self.df.iloc[idx]['Title']\n",
    "        description = self.df.iloc[idx]['Description']\n",
    "        curriculum = self.df.iloc[idx]['Curriculum']\n",
    "        \n",
    "        # Generate prompt\n",
    "        input_text = create_prompt(title, description, curriculum)\n",
    "        \n",
    "        # Tokenize input and output\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Set the target output as a summary or recommendation task\n",
    "        labels = self.tokenizer.encode_plus(\n",
    "            \"Summarize this course\",\n",
    "            max_length=100,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "dataset = CourseDataset(df, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Define training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Title: Introduction to Deep Learning Description: This course covers basic neural networks and deep learning. Curriculum: Week 1: Neural Networks, Week 2: Backpropagation, Week 3: CNNs Summarize this course or recommend it based on the description.\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a new prompt\n",
    "def generate_course_summary(model, tokenizer, title, description, curriculum, max_length=100):\n",
    "    model.eval()\n",
    "    input_text = create_prompt(title, description, curriculum)\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # Generate output (summary or recommendation)\n",
    "    summary_ids = model.generate(input_ids=input_ids, max_length=max_length, num_beams=2, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "title = \"Introduction to Deep Learning\"\n",
    "description = \"This course covers the basics of neural networks and deep learning.\"\n",
    "curriculum = \"Week 1: Neural Networks, Week 2: Backpropagation, Week 3: CNNs\"\n",
    "\n",
    "summary = generate_course_summary(model, tokenizer, title, description, curriculum)\n",
    "print(f\"Generated Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'courses_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     32\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep learning and neural networks\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m recommended_courses \u001b[38;5;241m=\u001b[39m recommend_courses(search_query, \u001b[43mcourses_df\u001b[49m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended Courses:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(recommended_courses)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'courses_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assume you have a dataframe `courses_df` with your courses data\n",
    "# Sample structure of courses_df:\n",
    "# courses_df = pd.DataFrame({\n",
    "#     'title': [\"Deep Learning\", \"Machine Learning\", \"Data Science\"],\n",
    "#     'description': [\"Learn deep learning...\", \"Understand machine learning...\", \"Introduction to data science...\"],\n",
    "#     'curriculum': [\"Curriculum for deep learning...\", \"Curriculum for machine learning...\", \"Curriculum for data science...\"]\n",
    "# })\n",
    "\n",
    "def recommend_courses(search_query, courses_df, top_n=3):\n",
    "    # Combine course titles and descriptions for vectorization\n",
    "    courses_df['combined'] = courses_df['title'] + \" \" + courses_df['description']\n",
    "\n",
    "    # Vectorize the search query and the courses\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(courses_df['combined'].tolist() + [search_query])\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "\n",
    "    # Get the indices of the top_n most similar courses\n",
    "    similar_indices = cosine_sim[0].argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Return recommended courses\n",
    "    recommendations = courses_df.iloc[similar_indices]\n",
    "    return recommendations[['title', 'description']]\n",
    "\n",
    "# Example usage\n",
    "search_query = \"deep learning and neural networks\"\n",
    "recommended_courses = recommend_courses(search_query, courses_df)\n",
    "\n",
    "print(\"Recommended Courses:\")\n",
    "print(recommended_courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (example with a CSV file containing course data)\n",
    "df = pd.read_csv('courses1.csv')  # Your dataset with course title, description, and curriculum\n",
    "\n",
    "# Function to create a search input (concatenate title, description, and curriculum)\n",
    "def create_prompt(title, description, curriculum):\n",
    "    return f\"{title} {description} {curriculum}\"\n",
    "\n",
    "# Search for the most relevant course\n",
    "def search_courses(df, query, top_n=5):\n",
    "    # Combine course information into a single text field for TF-IDF search\n",
    "    df['combined'] = df['title'] + ' ' + df['description'] + ' ' + df['curriculum']\n",
    "    \n",
    "    # Vectorize the combined course text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['combined'])\n",
    "    \n",
    "    # Vectorize the query using the same vectorizer\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    \n",
    "    # Compute cosine similarity between the query and all courses\n",
    "    similarity_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get top N related courses based on similarity\n",
    "    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "    recommendations = df.iloc[top_indices]\n",
    "    \n",
    "    return recommendations[['title', 'description', 'curriculum']]\n",
    "\n",
    "# Generate a course recommendation\n",
    "def generate_course_recommendation(query, df, top_n=5):\n",
    "    recommendations = search_courses(df, query, top_n)\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "query = \"Neural Networks and Deep Learning\"\n",
    "recommendations = generate_course_recommendation(query, df)\n",
    "\n",
    "print(f\"Top {len(recommendations)} recommended courses for '{query}':\\n\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
